\section{Introduction}
The transformer architecture invention led to the rise of various language models. One of the challenges the researchers have encountered with such models is the speed of autoregressive inference. Optimizing the inference is important whether aiming to provide a good user experience in real-time applications, saving on computational costs when running the model, or allowing running the model in an environment with limited computational and memory resources. 

In particular, memory bandwidth is a significant limitation in modern highly parallelizable hardware \cite{pope_efficiently_2022}\cite{shazeer_fast_2019}\cite{williams_roofline_2009}. In incremental decoding, most of the bandwidth is consumed by the constant loading of large key and value tensors \cite{shazeer_fast_2019}. To address this issue, Multi-Query Attention (MQA) architecture was proposed and was shown to increase the inference speed significantly though the generation quality was diminished \cite{shazeer_fast_2019}. To address the quality decline that MQA induces, Grouped-Query Attention (GQA) was introduced as an interpolation between the standard Multi-Head Attention (MHA) and MQA \cite{ainslie_gqa_2023}.

While Ainslie et al. \cite{ainslie_gqa_2023} present the GQA models' performance and speed results for different query groups, the models used in the experiments were pre-existing and then further uptrained. This research seeks to search for an optimal number of query groups relative to the number of attention heads at the pre-training stage to utilize the full potential of GQA. Moreover, the experiments conducted in this study focus on increasingly popular decoder-only models, where such optimizations play the most noticeable role due to the models' autoregressive nature. 

Besides, we are testing an orthogonal approach to reducing the size of key and value tensors, namely - by shrinking the dimensionality of key/query, and value vectors (kqv-vectors) while keeping the number of attention heads the same, to see the direct effect of their dimensionality on speed and quality. Even though Shazeer \cite{shazeer_fast_2019} has introduced this idea, the experiments have not revealed the direct correlation between the size of kqv-vectors and the models' quality, as with their size the number of attention heads was adjusted respectively. In addition, in those experiments for kqv-vectors' size, the speed measurements were omitted fully. 

Hence, in this study, we first try to find an optimal dimensionality for key/query and value vectors while keeping the number of attention heads the same. Next, we run GQA experiments with different proportions of groups relative to the total number of attention heads. In addition, once the optimal proportion of groups is determined, we investigate whether the quality and speed ratio relative to the baseline is preserved when adjusting the number of attention heads while keeping the proportion of groups constant. Finally, we are testing the combination of both approaches by setting the most optimal kqv-vectors' size in the GQA model with the best trade-offs. 

\begin{itemize}
    \item Main findings and how they contribute, and mention again that the goal is to find optimal trade-offs between speed and quality.
\end{itemize}

% \begin{itemize}
% \item Relate to the most relevant existing work from the literature (use BibTeX), explain their contributions, and (critically) indicate what is still unanswered. 
% %\emph{The existing state of the art describes the setup of general scientific papers, e.g.\ see~\cite{hengl2002rules}, but this may be different for computer science papers.}

% \item Explain what the research questions for this work are. 
% This usually is a subset of the unanswered questions. %\emph{The aim of this work is therefore to provide a good template specifically for papers in the field of computer science.}

% \item Summarize the main contributions/conclusions of this research.
% NB: Make sure the title of the paper is a good match to the main research question / contribution / conclusion.

% \item Briefly indicate how the rest of the paper fits together to answer the research question(s).
% \end{itemize}

% For a longer research paper, a section with a more elaborate discussion of the literature may follow, but for short (conference) submissions, this is often included in the introduction.

% Make sure the introduction and conclusion are easily understandable by everyone with a computer science bachelor (e.g.\ your examiner may have a completely different expertise).