\section{Background and Related Work}
\begin{itemize}
    \item Explain how Multi-Head Attention works in decoders
    \item Explain why reducing the size of key and value tensors is important in autoregressive decoding \cite{pope_efficiently_2022} \cite{shazeer_fast_2019} \cite{williams_roofline_2009} conceptually and using time/space complexity
    \item Explain what is MQA \cite{shazeer_fast_2019}
    \item Explain what is GQA and include a figure showcasing all three \cite{ainslie_gqa_2023}
    \item Mention the goal of reducing key and value vector sizes and that in the MQA paper it's not investigated properly 
    \item Mention again that GQA experiment used uptrained models and here we recreate it using pre-trained models
    \item Mention that no one tested yet how do MQA and GQA behave when given different number of attention heads (maybe \cite{michel_are_2019}) 
\end{itemize}
% Transformers is a state-of-the-art deep learning architecture in large language models used for data processing (encoding) and data generation tasks (decoding) \cite{vaswani_attention_2017}. Over time, transformer models have been scaled to enormous sizes with hundreds of billions of parameters to improve performance. This made the models less sustainable and more expensive both to train and operate due to the computational requirements induced by the scaling. 

% For this reason, the TinyStories dataset has specifically been designed to demonstrate the capability of improving the models' sample efficiency. It has been shown that improving the sample efficiency can significantly reduce the number of parameters while preserving the model's ability to generate coherent, diverse, and consistent results with the model demonstrating some reasoning capabilities \cite{eldan_tinystories_2023}. To facilitate the sample efficiency in models, the TinyStories was designed to be more cohesive and suitable for learning English grammar and vocabulary.

% Furthermore, the research reveals that the smaller models exhibit similar behaviour to their larger counterparts when it comes to architectural decisions \cite{eldan_tinystories_2023}. This, coupled with their suitability for limited-resources environments, positions them as ideal platforms for architectural experimentation in scenarios with restricted access to data and computational resources. 

% For some types of work in computer science the methodology is standard: analyze the problem (e.g., make assumptions and derive properties), present a new algorithm and its theoretical background, proving its correctness, and evaluate unproven aspects in simulation.
% Then an explanation of the methodology is often omitted, and the setup of the evaluation is part of a later section on the evaluation of the ideas.\footnote{This already shows that there is no single outline to be given for all papers.}
% In this case, explain relevant (background) concepts, theory and models in this section (with references) and relate them to your research question.
% Also this section then typically contains a more precise, formal description of the problem.

% Do not forget to give this section another name, for example after the problem you are solving.
