
@misc{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv:1409.0473 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICLR 2015 as oral presentation},
	file = {arXiv Fulltext PDF:/Users/a123/Zotero/storage/BAYXG54W/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/Users/a123/Zotero/storage/DI3SHM9Q/1409.html:text/html},
}

@misc{eldan_tinystories_2023,
	title = {{TinyStories}: {How} {Small} {Can} {Language} {Models} {Be} and {Still} {Speak} {Coherent} {English}?},
	shorttitle = {{TinyStories}},
	url = {http://arxiv.org/abs/2305.07759},
	abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Eldan, Ronen and Li, Yuanzhi},
	month = may,
	year = {2023},
	note = {arXiv:2305.07759 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/a123/Zotero/storage/DZB5VLV6/Eldan and Li - 2023 - TinyStories How Small Can Language Models Be and .pdf:application/pdf;arXiv.org Snapshot:/Users/a123/Zotero/storage/H22EL32Z/2305.html:text/html},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762v5},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/a123/Zotero/storage/22RFT3HJ/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/a123/Zotero/storage/HRGMQKAK/1706.html:text/html},
}

@misc{ainslie_gqa_2023,
	title = {{GQA}: {Training} {Generalized} {Multi}-{Query} {Transformer} {Models} from {Multi}-{Head} {Checkpoints}},
	shorttitle = {{GQA}},
	url = {http://arxiv.org/abs/2305.13245},
	abstract = {Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5\% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr√≥n, Federico and Sanghai, Sumit},
	month = dec,
	year = {2023},
	note = {arXiv:2305.13245 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: Accepted at EMNLP 2023. Added to related work},
	file = {Full Text:/Users/a123/Zotero/storage/S4IUDCKQ/Ainslie et al. - 2023 - GQA Training Generalized Multi-Query Transformer .pdf:application/pdf},
}

@misc{shazeer_fast_2019,
	title = {Fast {Transformer} {Decoding}: {One} {Write}-{Head} is {All} {You} {Need}},
	shorttitle = {Fast {Transformer} {Decoding}},
	url = {http://arxiv.org/abs/1911.02150},
	abstract = {Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.},
	urldate = {2024-04-02},
	publisher = {arXiv},
	author = {Shazeer, Noam},
	month = nov,
	year = {2019},
	note = {arXiv:1911.02150 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/a123/Zotero/storage/UWKHB86S/Shazeer - 2019 - Fast Transformer Decoding One Write-Head is All Y.pdf:application/pdf;arXiv.org Snapshot:/Users/a123/Zotero/storage/MIB6PWV9/1911.html:text/html},
}

@misc{choshen_call_2024,
	title = {[{Call} for {Papers}] {The} 2nd {BabyLM} {Challenge}: {Sample}-efficient pretraining on a developmentally plausible corpus},
	shorttitle = {[{Call} for {Papers}] {The} 2nd {BabyLM} {Challenge}},
	url = {http://arxiv.org/abs/2404.06214},
	abstract = {After last year's successful BabyLM Challenge, the competition will be hosted again in 2024/2025. The overarching goals of the challenge remain the same; however, some of the competition rules will be different. The big changes for this year's competition are as follows: First, we replace the loose track with a paper track, which allows (for example) non-model-based submissions, novel cognitively-inspired benchmarks, or analysis techniques. Second, we are relaxing the rules around pretraining data, and will now allow participants to construct their own datasets provided they stay within the 100M-word or 10M-word budget. Third, we introduce a multimodal vision-and-language track, and will release a corpus of 50\% text-only and 50\% image-text multimodal data as a starting point for LM model training. The purpose of this CfP is to provide rules for this year's challenge, explain these rule changes and their rationale in greater detail, give a timeline of this year's competition, and provide answers to frequently asked questions from last year's challenge.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Choshen, Leshem and Cotterell, Ryan and Hu, Michael Y. and Linzen, Tal and Mueller, Aaron and Ross, Candace and Warstadt, Alex and Wilcox, Ethan and Williams, Adina and Zhuang, Chengxu},
	month = apr,
	year = {2024},
	note = {arXiv:2404.06214 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/a123/Zotero/storage/SPQZREF9/Choshen et al. - 2024 - [Call for Papers] The 2nd BabyLM Challenge Sample.pdf:application/pdf;arXiv.org Snapshot:/Users/a123/Zotero/storage/T77EIARH/2404.html:text/html},
}

@misc{warstadt_call_2023,
	title = {Call for {Papers} -- {The} {BabyLM} {Challenge}: {Sample}-efficient pretraining on a developmentally plausible corpus},
	shorttitle = {Call for {Papers} -- {The} {BabyLM} {Challenge}},
	url = {http://arxiv.org/abs/2301.11796},
	abstract = {We present the call for papers for the BabyLM Challenge: Sample-efficient pretraining on a developmentally plausible corpus. This shared task is intended for participants with an interest in small scale language modeling, human language acquisition, low-resource NLP, and cognitive modeling. In partnership with CoNLL and CMCL, we provide a platform for approaches to pretraining with a limited-size corpus sourced from data inspired by the input to children. The task has three tracks, two of which restrict the training data to pre-released datasets of 10M and 100M words and are dedicated to explorations of approaches such as architectural variations, self-supervised objectives, or curriculum learning. The final track only restricts the amount of text used, allowing innovation in the choice of the data, its domain, and even its modality (i.e., data from sources other than text is welcome). We will release a shared evaluation pipeline which scores models on a variety of benchmarks and tasks, including targeted syntactic evaluations and natural language understanding.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Warstadt, Alex and Choshen, Leshem and Mueller, Aaron and Williams, Adina and Wilcox, Ethan and Zhuang, Chengxu},
	month = jan,
	year = {2023},
	note = {arXiv:2301.11796 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/a123/Zotero/storage/IPTN7UM7/Warstadt et al. - 2023 - Call for Papers -- The BabyLM Challenge Sample-ef.pdf:application/pdf;arXiv.org Snapshot:/Users/a123/Zotero/storage/UN8M4D4P/2301.html:text/html},
}

@misc{pope_efficiently_2022,
	title = {Efficiently {Scaling} {Transformer} {Inference}},
	url = {http://arxiv.org/abs/2211.05102},
	abstract = {We study the problem of efficient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efficiency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32x larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76\% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Levskaya, Anselm and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
	month = nov,
	year = {2022},
	note = {arXiv:2211.05102 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/a123/Zotero/storage/5G6NMQKY/Pope et al. - 2022 - Efficiently Scaling Transformer Inference.pdf:application/pdf;arXiv.org Snapshot:/Users/a123/Zotero/storage/UYSPEHWI/2211.html:text/html},
}

@misc{de_jong_fido_2023,
	title = {{FiDO}: {Fusion}-in-{Decoder} optimized for stronger performance and faster inference},
	shorttitle = {{FiDO}},
	url = {http://arxiv.org/abs/2212.08153},
	abstract = {Fusion-in-Decoder (FiD) is a powerful retrieval-augmented language model that sets the state-of-the-art on many knowledge-intensive NLP tasks. However, the architecture used for FiD was chosen by making minimal modifications to a standard T5 model, which our analysis shows to be highly suboptimal for a retrieval-augmented model. In particular, FiD allocates the bulk of FLOPs to the encoder, while the majority of inference time results from memory bandwidth constraints in the decoder. We propose two simple changes to the FiD architecture to alleviate memory bandwidth constraints, and speed up inference by 7x. This allows us to use a much larger decoder at modest cost. We denote FiD with the above modifications as FiDO, and show that it strongly improves performance over existing FiD models for a wide range of inference budgets. For example, FiDO-Large-XXL performs faster inference than FiD-Base and achieves better performance than FiD-Large.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {de Jong, Michiel and Zemlyanskiy, Yury and Ainslie, Joshua and FitzGerald, Nicholas and Sanghai, Sumit and Sha, Fei and Cohen, William},
	month = jun,
	year = {2023},
	note = {arXiv:2212.08153 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: ACL Findings 2023},
	file = {arXiv Fulltext PDF:/Users/a123/Zotero/storage/GVYGT37T/de Jong et al. - 2023 - FiDO Fusion-in-Decoder optimized for stronger per.pdf:application/pdf;arXiv.org Snapshot:/Users/a123/Zotero/storage/6YUPWDBV/2212.html:text/html},
}

@article{williams_roofline_2009,
	title = {Roofline: an insightful visual performance model for multicore architectures},
	volume = {52},
	issn = {0001-0782, 1557-7317},
	shorttitle = {Roofline},
	url = {https://dl.acm.org/doi/10.1145/1498765.1498785},
	doi = {10.1145/1498765.1498785},
	abstract = {The Roofline model offers insight on how to improve the performance of software and hardware.},
	language = {en},
	number = {4},
	urldate = {2024-05-01},
	journal = {Communications of the ACM},
	author = {Williams, Samuel and Waterman, Andrew and Patterson, David},
	month = apr,
	year = {2009},
	pages = {65--76},
	file = {Full Text:/Users/a123/Zotero/storage/T8644BR9/Williams et al. - 2009 - Roofline an insightful visual performance model f.pdf:application/pdf},
}

@misc{michel_are_2019,
	title = {Are {Sixteen} {Heads} {Really} {Better} than {One}?},
	url = {http://arxiv.org/abs/1905.10650},
	abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Michel, Paul and Levy, Omer and Neubig, Graham},
	month = nov,
	year = {2019},
	note = {arXiv:1905.10650 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: NeurIPS 2019},
	file = {arXiv Fulltext PDF:/Users/a123/Zotero/storage/7NH5SMT5/Michel et al. - 2019 - Are Sixteen Heads Really Better than One.pdf:application/pdf;arXiv.org Snapshot:/Users/a123/Zotero/storage/PTBU8BAH/1905.html:text/html},
}

@inproceedings{zhang_accelerating_2018,
	address = {Melbourne, Australia},
	title = {Accelerating {Neural} {Transformer} via an {Average} {Attention} {Network}},
	url = {http://aclweb.org/anthology/P18-1166},
	doi = {10.18653/v1/P18-1166},
	language = {en},
	urldate = {2024-05-01},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong},
	year = {2018},
	pages = {1789--1798},
	file = {Full Text:/Users/a123/Zotero/storage/2S78KHLI/Zhang et al. - 2018 - Accelerating Neural Transformer via an Average Att.pdf:application/pdf},
}

@inproceedings{povey_time-restricted_2018,
	address = {Calgary, AB},
	title = {A {Time}-{Restricted} {Self}-{Attention} {Layer} for {ASR}},
	isbn = {978-1-5386-4658-8},
	url = {https://ieeexplore.ieee.org/document/8462497/},
	doi = {10.1109/ICASSP.2018.8462497},
	urldate = {2024-05-02},
	booktitle = {2018 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Povey, Daniel and Hadian, Hossein and Ghahremani, Pegah and Li, Ke and Khudanpur, Sanjeev},
	month = apr,
	year = {2018},
	pages = {5874--5878},
}

@inproceedings{warstadt_findings_2023,
	address = {Singapore},
	title = {Findings of the {BabyLM} {Challenge}: {Sample}-{Efficient} {Pretraining} on {Developmentally} {Plausible} {Corpora}},
	shorttitle = {Findings of the {BabyLM} {Challenge}},
	url = {https://aclanthology.org/2023.conll-babylm.1},
	doi = {10.18653/v1/2023.conll-babylm.1},
	language = {en},
	urldate = {2024-05-06},
	booktitle = {Proceedings of the {BabyLM} {Challenge} at the 27th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Warstadt, Alex and Mueller, Aaron and Choshen, Leshem and Wilcox, Ethan and Zhuang, Chengxu and Ciro, Juan and Mosquera, Rafael and Paranjabe, Bhargavi and Williams, Adina and Linzen, Tal and Cotterell, Ryan},
	year = {2023},
	pages = {1--6},
	file = {Full Text:/Users/a123/Zotero/storage/KZ6XCAU3/Warstadt et al. - 2023 - Findings of the BabyLM Challenge Sample-Efficient.pdf:application/pdf},
}

@misc{dettmers_llmint8_2022,
	title = {{LLM}.int8(): 8-bit {Matrix} {Multiplication} for {Transformers} at {Scale}},
	shorttitle = {{LLM}.int8()},
	url = {http://arxiv.org/abs/2208.07339},
	abstract = {Large language models have been widely adopted but require significant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We first use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open-source our software.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
	month = nov,
	year = {2022},
	note = {arXiv:2208.07339 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Published at NeurIPS 2022. Camera-ready version},
	file = {arXiv Fulltext PDF:/Users/a123/Zotero/storage/NEFBI65X/Dettmers et al. - 2022 - LLM.int8() 8-bit Matrix Multiplication for Transf.pdf:application/pdf;arXiv.org Snapshot:/Users/a123/Zotero/storage/UNZV7XFK/2208.html:text/html},
}

@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv Fulltext PDF:/Users/a123/Zotero/storage/2DHQRZMI/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/Users/a123/Zotero/storage/NUCHDWT4/1503.html:text/html},
}
